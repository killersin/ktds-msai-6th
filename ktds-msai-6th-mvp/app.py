import os
import json
import time
import logging
import streamlit as st
from dotenv import load_dotenv

load_dotenv()

# Application Insights ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
try:
    conn_str = os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING")
    if conn_str:
        try:
            from opencensus.ext.azure.log_exporter import AzureLogHandler
            logger.addHandler(AzureLogHandler(connection_string=conn_str))
            logger.info("Application Insights ì—°ê²° ì„±ê³µ!")
        except Exception as e:
            # íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜ ë˜ëŠ” í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ë¬¸ì œë¥¼ ë¡œì»¬ ë¡œê±°ë¡œ ê¸°ë¡
            logger.warning(f"Application Insights í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
except Exception:
    pass

st.set_page_config(page_title="KTDS MSAI MVP 616", page_icon="ğŸ›¡ï¸")

if "show_board" not in st.session_state:
    st.session_state["show_board"] = False

if st.sidebar.button("í™ˆìœ¼ë¡œ"):
    st.session_state["show_board"] = False
if st.sidebar.button("ê²Œì‹œê¸€ ë³´ê¸°"):
    st.session_state["show_board"] = True

# --- ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ (ê²Œì‹œê¸€ ë³´ê¸° ë²„íŠ¼ê³¼ ëª¨ë“œ ì„ íƒ ì‚¬ì´) ---
UPLOAD_DIR = os.path.join("data", "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

uploaded_files = st.sidebar.file_uploader("íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ê°œ ê°€ëŠ¥)", accept_multiple_files=True)
if uploaded_files:
    if "uploaded_files" not in st.session_state:
        st.session_state["uploaded_files"] = []
    for f in uploaded_files:
        # ì €ì¥ ê²½ë¡œ ê²°ì • (ì¤‘ë³µ ë°©ì§€)
        save_name = f.name
        save_path = os.path.join(UPLOAD_DIR, save_name)
        if os.path.exists(save_path):
            base, ext = os.path.splitext(save_name)
            save_name = f"{base}_{int(time.time())}{ext}"
            save_path = os.path.join(UPLOAD_DIR, save_name)
        try:
            with open(save_path, "wb") as out:
                out.write(f.getbuffer())
            #st.sidebar.success(f"ì €ì¥ ì™„ë£Œ: {save_name}")
            meta = {"name": save_name, "path": save_path, "type": f.type, "size": os.path.getsize(save_path)}
            st.session_state["uploaded_files"].append(meta)

            # Azure Blob Storageì— ì—…ë¡œë“œ ì‹œë„
            conn_str = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
            if conn_str:
                try:
                    from azure.storage.blob import BlobServiceClient
                    blob_service = BlobServiceClient.from_connection_string(conn_str)
                    container_name = "compliance"
                    try:
                        container_client = blob_service.create_container(container_name)
                    except Exception:
                        container_client = blob_service.get_container_client(container_name)

                    blob_client = blob_service.get_blob_client(container=container_name, blob=save_name)
                    with open(save_path, "rb") as data:
                        blob_client.upload_blob(data, overwrite=True)
                    #st.sidebar.success(f"Blob ì—…ë¡œë“œ ì„±ê³µ: {container_name}/{save_name}")
                    meta["blob_url"] = blob_client.url
                except Exception as e:
                    st.sidebar.error(f"Blob ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                st.sidebar.warning("AZURE_STORAGE_CONNECTION_STRINGì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ Blob ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

            # ìë™ ì¸ë±ì‹±: ì—…ë¡œë“œëœ íŒŒì¼ì´ JSONì´ë©´ Azure Search ì¸ë±ìŠ¤ì— ìƒ‰ì¸ ì‹œë„
            try:
                if save_path.lower().endswith('.json'):
                    try:
                        from modules.azure_ai_search import AzureSearchClient
                        # ì‚¬ì´ë“œë°”ì™€ ë©”ì¸ì— ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ìˆëŠ” placeholder ì‚¬ìš©
                        sidebar_ph = st.sidebar.empty()
                        main_ph = st.empty()

                        sidebar_ph.info("ì—…ë¡œë“œëœ JSONì„ Azure Searchì— ì¸ë±ì‹± ì¤‘ì…ë‹ˆë‹¤...")
                        asc = AzureSearchClient()
                        asc.ensure_index_exists()
                        res = asc.index_from_file(file_path=save_path)
                        msg = f"ì¸ë±ì‹± ì™„ë£Œ: ì´={res.get('total')}, ì„±ê³µ={res.get('success')}, ì‹¤íŒ¨={res.get('failed')}"

                        # ì ê¹ ë©”ì¸ì— í‘œì‹œí•˜ê³  ìë™ìœ¼ë¡œ ì œê±°
                        #main_ph.success(f"íŒŒì¼ ì¸ë±ì‹± ì„±ê³µ: {save_name} â€” {msg}")
                        # ì‚¬ì´ë“œë°”ì—ë„ ê²°ê³¼ë¥¼ ì ì‹œ ë³´ì—¬ì¤¬ë‹¤ê°€ ì§€ì›€
                        sidebar_ph.success(msg)
                        time.sleep(3)
                        try:
                            main_ph.empty()
                        except Exception:
                            pass
                        try:
                            sidebar_ph.empty()
                        except Exception:
                            pass
                    except ValueError as ve:
                        # í™˜ê²½ë³€ìˆ˜ ëˆ„ë½ìœ¼ë¡œ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
                        st.sidebar.warning(f"ì¸ë±ìŠ¤ ì´ˆê¸°í™” ê±´ë„ˆëœ€: {ve}")
                    except Exception as ie:
                        st.sidebar.error(f"ì¸ë±ì‹± ì˜ˆì™¸ ë°œìƒ: {ie}")
            except Exception:
                # ì•ˆì „ì„ ìœ„í•´ ì „ì²´ ë¸”ë¡ì˜ ì˜ˆì™¸ë¥¼ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                pass

        except Exception as e:
            st.sidebar.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ë³´ê¸° í† ê¸€
# if st.sidebar.checkbox("ì—…ë¡œë“œ íŒŒì¼ ëª©ë¡ ë³´ê¸°"):
#     files = st.session_state.get("uploaded_files", [])
#     if not files:
#         st.sidebar.info("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         for i, mf in enumerate(files):
#             st.sidebar.write(f"{i+1}. {mf['name']} â€” {mf['size']} bytes")

mode = st.sidebar.selectbox("ëª¨ë“œ ì„ íƒ", ["Azure Search", "ì±—ë´‡"])

# ëª¨ë“œ ë³€ê²½ ì‹œ ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ì´ˆê¸°í™”
if "last_mode" not in st.session_state:
    st.session_state["last_mode"] = mode
elif st.session_state["last_mode"] != mode:
    # ëª¨ë“œê°€ ë°”ë€Œë©´ ì´ì „ ì„¸ì…˜ ë©”ì‹œì§€ë¥¼ ì´ˆê¸°í™”í•˜ì—¬ í˜¼ë™ì„ ë°©ì§€
    st.session_state["messages"] = []
    st.session_state["last_mode"] = mode

# ê²Œì‹œê¸€ ëª¨ë“ˆ ë¶„ë¦¬ í˜¸ì¶œ
if st.session_state["show_board"]:
    try:
        from modules.newssummary import show_board
        show_board()
    except Exception as e:
        st.error(f"ê²Œì‹œê¸€ ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    raise SystemExit  # ê²Œì‹œíŒ í™”ë©´ë§Œ ë³´ì—¬ì£¼ê³  ì¢…ë£Œ (ì´í›„ ì½”ë“œëŠ” ì‹¤í–‰í•˜ì§€ ì•ŠìŒ)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ---
# í™˜ê²½ë³€ìˆ˜ í‚¤ë¥¼ ì½ì–´ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
# ë°˜í™˜ê°’ ì˜ˆì‹œ: {"search_endpoint": "...", "search_key": "...", "search_index": "...", ...}
def _get_env_keys():
    return {
        "search_endpoint": os.getenv("AZURE_SEARCH_ENDPOINT"),
        "search_key": os.getenv("AZURE_SEARCH_API_KEY"),
        "search_index": os.getenv("AZURE_SEARCH_INDEX_NAME"),
        "embedding_deployment": os.getenv("AZURE_EMBEDDING_DEPLOYMENT"),
        "chat_deployment": "gpt-4.1-mini",
        "azure_endpoint": os.getenv("AZURE_ENDPOINT"),
        "openai_key": os.getenv("OPENAI_API_KEY"),
        "openai_version": os.getenv("OPENAI_API_VERSION")
    }

# Azure Searchìš© SearchClientë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
# ì¸ì: endpoint(ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸), key(ê²€ìƒ‰ ì„œë¹„ìŠ¤ í‚¤), index(ì¸ë±ìŠ¤ ì´ë¦„)
# ë°˜í™˜: SearchClient ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ None
def _init_search_client(endpoint, key, index):
    if not (endpoint and key and index):
        return None
    try:
        from azure.search.documents import SearchClient
        from azure.core.credentials import AzureKeyCredential
        return SearchClient(endpoint=endpoint, index_name=index, credential=AzureKeyCredential(key))
    except Exception:
        return None

# ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ Azure OpenAI ì„ë² ë”©ì„ ìš”ì²­í•˜ì—¬ ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
# ì¸ì: prompt(í…ìŠ¤íŠ¸), deployment(ì„ë² ë”© ëª¨ë¸ ì´ë¦„), env(í™˜ê²½ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬)
# ë°˜í™˜: embedding ë²¡í„°(list) ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
def _get_embedding(prompt, deployment, env):
    if not deployment:
        return None
    try:
        import importlib
        oa_mod = importlib.import_module("azure.ai.openai")
        OpenAIClient = getattr(oa_mod, "OpenAIClient")
        from azure.core.credentials import AzureKeyCredential as CoreAzureKey
        if not (env["openai_key"] and env["azure_endpoint"]):
            return None
        oa_client = OpenAIClient(env["azure_endpoint"], CoreAzureKey(env["openai_key"]))
        emb_resp = oa_client.embeddings.create(model=deployment, input=prompt)
        return emb_resp.data[0].embedding
    except Exception:
        return None

# Azure Searchì—ì„œ í”„ë¡¬í”„íŠ¸(ë˜ëŠ” ì„ë² ë”©)ë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
# ì¸ì: search_client, prompt, embedding_vector, top_k
# ë°˜í™˜: ë¬¸ì„œ dict ë¦¬ìŠ¤íŠ¸ (id, domain, category, content, score ë“±)
def _retrieve_documents(search_client, prompt, embedding_vector, top_k):
    docs = []
    if not search_client:
        return docs
    try:
        if embedding_vector is not None:
            try:
                results = search_client.search(search_text="*", vector={"value": embedding_vector, "fields": "content_vector", "k": top_k})
            except TypeError:
                results = search_client.search(search_text="", vector={"value": embedding_vector, "fields": "content_vector", "k": top_k})
        else:
            results = search_client.search(search_text=prompt, top=top_k)

        for r in results:
            docs.append({
                "id": r.get("id") or r.get("@search.documentId"),
                "domain": r.get("domain"),
                "category": r.get("category") or r.get("title") or "",
                "content": r.get("content") or r.get("text") or "",
                "score": r.get("@search.score")
            })
    except Exception as e:
        st.error(f"Azure Search ì¡°íšŒ ì‹¤íŒ¨: {e}")
    return docs

def _build_context_text(retrieved_docs):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ëª¨ë¸ì— ì£¼ì…í•  ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ê° ë¬¸ì„œë§ˆë‹¤ ì¶œì²˜ í—¤ë”ë¥¼ ë¶™ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not retrieved_docs:
        return ""
    parts = []
    for i, d in enumerate(retrieved_docs):
        header = f"[ì¶œì²˜ {i+1}] "
        if d.get("domain"):
            header += d.get("domain") + " | "
        parts.append(header + d.get("content", ""))
    return "\n\n".join(parts)

def _inject_context_into_messages(messages, context_text):
    """
    ê¸°ì¡´ ë©”ì‹œì§€ ëª©ë¡ì— ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì‚½ì…í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    context_textê°€ ë¹ˆ ê²½ìš° ì›ë³¸ ë©”ì‹œì§€ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not context_text:
        return messages
    system_context = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ ì£¼ì‹íšŒì‚¬ KTì˜ ì»´í”Œë¼ì´ì–¸ìŠ¤ ë‹´ë‹¹ìì…ë‹ˆë‹¤.\n"
            "ì‚¬ìš©ìë“¤ì˜ ì»´í”Œë¼ì´ì–¸ìŠ¤ ê´€ë ¨ ì§ˆì˜ì— ë‹µë³€ì„ ë„ì™€ì£¼ëŠ” helpful assistantì…ë‹ˆë‹¤.\n"
            "ì»´í”Œë¼ì´ì–¸ìŠ¤ëŠ” ê¸°ì—…í™œë™ì—ì„œ ë²Œì–´ì§ˆ ìˆ˜ ìˆëŠ” ë²•ë¥ ì , ìœ¤ë¦¬ì , ì¬ë¬´ì , ë¹„ì¬ë¬´ì  ìœ„í—˜ìš”ì†Œë¥¼ ì‚¬ì „ì— ì–µì œí•˜ê³  ë°©ì§€í•˜ëŠ” í™œë™ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n"
            "í™•ì‹¤í•˜ì§€ ì•Šê±°ë‚˜ ë‹µë³€ì´ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ëª…ì‹œí•˜ì„¸ìš”.\n\n"
        ) + context_text
    }
    msgs = [m for m in messages]
    insert_pos = max(0, len(msgs) - 1)
    msgs.insert(insert_pos, system_context)
    return msgs

# ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ì•„ Streamlit ì±„íŒ… UIì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•˜ê³  ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
# ì¸ì: model(ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸ ë˜í¼), messages_for_model(ëª¨ë¸ì— ì „ë‹¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸)
# ë°˜í™˜: ëª¨ë¸ì´ ìƒì„±í•œ ì „ì²´ ì‘ë‹µ ë¬¸ìì—´
def _stream_response_to_chat(model, messages_for_model):
    response_text = ""
    with st.chat_message("assistant"):
        placeholder = st.empty()
        try:
            for chunk in model.stream(messages_for_model):
                response_text += chunk.content
                placeholder.markdown(response_text)
        except Exception as e:
            st.error(f"ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    return response_text

# LangChain ê¸°ë°˜ AzureChatOpenAI ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
# ì¸ì: env(í™˜ê²½ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬), deployment(ì±— ëª¨ë¸ ë°°í¬ ì´ë¦„)
# ë°˜í™˜: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
def _init_chat_model(env, deployment):
    try:
        from langchain_openai import AzureChatOpenAI
        return AzureChatOpenAI(
            azure_endpoint=env["azure_endpoint"],
            api_key=env["openai_key"],
            api_version=env["openai_version"],
            azure_deployment=deployment
        )
    except Exception as e:
        st.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# ë¡œì»¬ JSONì—ì„œ ì¹´í…Œê³ ë¦¬ëª…ì„ ì½ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
# ë°˜í™˜ê°’ì—ëŠ” ì›ë³¸ 'ë²ˆí˜¸. ì´ë¦„'ê³¼ ë²ˆí˜¸ë¥¼ ì œê±°í•œ ë‹¨ì¶•ëª…(ì˜ˆ: 'ì‚°ì—…ì•ˆì „ë³´ê±´')ì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.
def _load_local_categories(path="data/9_field.json"):
    cats = []
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, list):
                for item in data:
                    c = item.get("category")
                    if c:
                        cats.append(c)
                        # ìˆ«ì ì ‘ë‘ì‚¬ ì œê±° (ì˜ˆ: '7. ì‚°ì—…ì•ˆì „ë³´ê±´' -> 'ì‚°ì—…ì•ˆì „ë³´ê±´')
                        short = c
                        if "." in c:
                            short = c.split(".", 1)[1].strip()
                        cats.append(short)
    except Exception:
        # ì‹¤íŒ¨í•´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []
    return list(dict.fromkeys([c for c in cats if c]))


def _format_messages_for_slack(messages):
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜: ì—­í•  êµ¬ë¶„ê³¼ ë‚´ìš©
    parts = []
    for m in messages:
        role = m.get("role", "user")
        content = m.get("content", "")
        parts.append(f"[{role.upper()}] {content}")
    return "\n\n".join(parts)


def _send_to_slack(messages):
    webhook = os.getenv("SLACK_WEBHOOK_URL")
    payload_text = _format_messages_for_slack(messages)
    if not webhook:
        st.warning("SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ì— Webhook URLì„ ì¶”ê°€í•˜ì„¸ìš”.")
        # ëŒ€ì‹  í¬ë§·ëœ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ëª¨ë‹¬ì²˜ëŸ¼ í‘œê¸°
        st.code(payload_text)
        return
    try:
        import requests
        resp = requests.post(webhook, json={"text": payload_text}, timeout=5)
        if resp.status_code == 200:
            st.success("ëŒ€í™” ë‚´ìš©ì„ Slackìœ¼ë¡œ ì „ì†¡í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.error(f"Slack ì „ì†¡ ì‹¤íŒ¨: {resp.status_code} {resp.text}")
    except Exception as e:
        st.error(f"Slack ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")


# --- ê³µí†µ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì—†ì´ ë¹ˆ ëŒ€í™” ì´ë ¥ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
    st.session_state["messages"] = []
if "rag_top_k" not in st.session_state:
    st.session_state["rag_top_k"] = 10

# í™”ë©´ ë Œë”ë§
env = _get_env_keys()

# ë¡œì»¬ JSONì—ì„œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë¯¸ë¦¬ ë¡œë“œ
LOCAL_CATEGORIES = _load_local_categories()

if mode == "Azure Search":
    if not (env["search_endpoint"] and env["search_key"] and env["search_index"]):
        st.info("Azure Search ì„¤ì •ì´ .envì— ì—†ìŠµë‹ˆë‹¤. AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_API_KEY, AZURE_SEARCH_INDEX_NAMEì„ ì„¤ì •í•˜ì„¸ìš”.")
    else:
        st.title("ğŸ˜€ì»´í”Œë¼ì´ì–¸ìŠ¤ ì±—ë´‡")
        st.caption("Azure AI Searchì™€ OpenAI GPT-4.1-minië¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ì»´í”Œë¼ì´ì–¸ìŠ¤ RAG ì±—ë´‡ì…ë‹ˆë‹¤.")
        model = _init_chat_model(env, env["chat_deployment"])
        if not model:
            st.error("ì±—ë´‡ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # íˆìŠ¤í† ë¦¬ í‘œì‹œ
            for msg in st.session_state["messages"]:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])

            if prompt := st.chat_input("User : "):
                st.session_state["messages"].append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                    search_client = _init_search_client(env["search_endpoint"], env["search_key"], env["search_index"])
                    embedding_vector = _get_embedding(prompt, env["embedding_deployment"], env)
                    top_k = int(st.session_state.get("rag_top_k", 5))
                    # ì¹´í…Œê³ ë¦¬ ê°œìš” ìš”ì²­ íŒë³„: ë¡œì»¬ ì¹´í…Œê³ ë¦¬ ì´ë¦„ì´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ê²½ìš°
                    is_category_overview = any(cat in prompt for cat in LOCAL_CATEGORIES)

                    if is_category_overview:
                        # agg ë¬¸ì„œ(item_index == -1)ë¥¼ ìš°ì„  ì¡°íšŒí•˜ì—¬ ì¹´í…Œê³ ë¦¬ ì„¤ëª…ì„ í™•ë³´
                        retrieved_docs = _retrieve_documents(search_client, prompt, embedding_vector, top_k)
                        # í•„í„°ë§ ì—†ì´ ì´ë¯¸ indexì—ì„œ agg_docê°€ ìƒì„±ë˜ì–´ ìˆìœ¼ë©´ í¬í•¨ë˜ì–´ì•¼ í•¨
                    else:
                        retrieved_docs = _retrieve_documents(search_client, prompt, embedding_vector, top_k)

                    if not retrieved_docs:
                        canned = "ì»´í”Œë¼ì´ì–¸ìŠ¤ ê´€ë ¨ ë¬¸ì˜ì— ëŒ€í•´ì„œë§Œ ë‹µë³€ì„ ì œê³µí•˜ê³  ìˆìŒì„ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤.\nê·¸ ì™¸ì˜ ë¬¸ì˜ì‚¬í•­ì€ ë‹µë³€ì´ ì–´ë ¤ìš´ ì  ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
                        st.info(canned)
                        # ëª¨ë¸ì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ê³ ì • ì‘ë‹µì„ ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
                        st.session_state["messages"].append({"role": "assistant", "content": canned})
                    else:
                        st.subheader(f"ê²€ìƒ‰ ê²°ê³¼ ({len(retrieved_docs)})")
                        for d in retrieved_docs:
                            title_parts = [p for p in (d.get("domain"), d.get("category")) if p]
                            title = " | ".join(title_parts) if title_parts else (d.get("category") or "í•­ëª©")
                            with st.expander(f"{title} â€” {d.get('score')}"):
                                content = (d.get("content") or "").lstrip()  # ì„ í–‰ ê³µë°± ì œê±°
                                st.text(content)  # ë˜ëŠ” st.write(content) / st.markdown(content) ëŒ€ì‹  st.text ì‚¬ìš©

                        context_text = _build_context_text(retrieved_docs)
                        messages_for_model = _inject_context_into_messages(st.session_state["messages"], context_text)

                        response_text = _stream_response_to_chat(model, messages_for_model)

                        
else:
    # ì±—ë´‡ ê¸°ë³¸ í™”ë©´ (RAG ì—†ìŒ)
    st.title("ktds-msai-6th-mvp ğŸ¤–")
    st.caption("Azure OpenAIì˜ ìµœì‹  GPT-4.1-mini ëª¨ë¸ì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ì…ë‹ˆë‹¤.")
    model = _init_chat_model(env, "gpt-4.1-mini")
    if not model:
        st.error("ì±—ë´‡ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
        for msg in st.session_state["messages"]:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    if prompt := st.chat_input("User : "):
        # ìœ ì € ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
        st.session_state["messages"].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ ë° ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
        response_text = ""
        try:
            response_text = _stream_response_to_chat(model, st.session_state["messages"])
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        # ìŠ¤íŠ¸ë¦¬ë°ëœ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì„ ëŒ€í™” ì´ë ¥ì— ì¶”ê°€í•˜ì—¬ ë‹¤ìŒ ë Œë”ë§ ì‹œ ë³´ì¡´
        st.session_state["messages"].append({"role": "assistant", "content": response_text})
